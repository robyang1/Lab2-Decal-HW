{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "330f5735",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"longshort_strategy part 2.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efbb5548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce7db535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isclose(value, original, tolerance= 0.05):\n",
    "    return value <= original * (1+tolerance) and value >= original * (1-tolerance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12c5e0f-171d-4868-8595-4a45c638e667",
   "metadata": {},
   "source": [
    "\n",
    "### Introduction\n",
    "\n",
    "In this homework, we will explore the foundational tools and techniques required to develop and evaluate a trading strategy. Specifically, we will:\n",
    "\n",
    "- **Learn and apply Pandas** for data cleaning and preprocessing.\n",
    "- **Implement a basic trading strategy**, including both long and short positions.\n",
    "- **Model the costs** associated with executing the strategy, such as transaction fees or slippage.\n",
    "- **Calculate expected returns** to assess the potential profitability of the strategy.\n",
    "- **Incorporate expected returns** into an enhanced trading strategy for improved performance.\n",
    "- **Experiment with predictive features and signals** to refine the strategy further.\n",
    "\n",
    "By the end of this homework, you will have a solid understanding of how to use data-driven methods to design and evaluate tradin\n",
    "\n",
    "**Note: For the Spring 2025 semester, we only recommend you complete up to section 7 (\"evaluating performance\").** The rest of the test cases have not been updated for new data and will likely fail, but you are welcome to work on further sections and ignore the test cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe1e415",
   "metadata": {},
   "source": [
    "# returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4c4193-f892-4726-ba8c-5dd7b47a69e3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Calculating Return on Investment (ROI) in Finance\n",
    "\n",
    "To evaluate the performance of any strategy, it is essential to calculate the **return on investment (ROI)**. In finance, we often use **log returns** instead of **simple returns** because of their mathematical advantages.\n",
    "\n",
    "#### Why Use Log Returns?\n",
    "- **Ease of Calculations**: Log returns are simpler to work with in mathematical models.\n",
    "- **Constant Compounding**: Log returns represent continuously compounding returns, allowing us to **add them over time** instead of multiplying.\n",
    "\n",
    "#### How to Calculate Returns\n",
    "1. **Simple Returns**:  \n",
    "   \n",
    "   $$\\text{Simple Return} = \\frac{x_1 - x_0}{x_0}$$\n",
    "   \n",
    "   This is the **percentage change** from \\(x_0\\) (initial value) to \\(x_1\\) (final value).\n",
    "\n",
    "2. **Log Returns**:  \n",
    "   \n",
    "   $$\\text{Log Return} = \\log\\left(\\frac{x_1}{x_0}\\right)$$\n",
    "   \n",
    "   This is the natural logarithm of the ratio between the final and initial values.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90051a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if you can derive the transformations from log returns to simple returns, and simple returns to log returns\n",
    "hdf = pd.read_parquet('./stock_data.parquet')\n",
    "hdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2c6454",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use np.exp\n",
    "def logtosimple(logreturn):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dafea5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdab521",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use np.log\n",
    "def simpletolog(simplereturn):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c8dbd3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53958da2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's look at an example asset\n",
    "# create a dataframe that is just the adjusted close and volume for AAL\n",
    "aal = hdf.loc[:,hdf.columns.get_level_values(1) == ...]\n",
    "aal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee424941",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90402543-b9fc-47e2-8f43-72c8a5e18af8",
   "metadata": {},
   "source": [
    "Let's remove the top level of the columns in order to make things easier - we already know the symbol is AAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0588dda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aal.columns = aal.columns.get_level_values(0)\n",
    "aal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd6337c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f331889a-5535-4a27-8b37-543ba37a84a3",
   "metadata": {},
   "source": [
    "To calculate returns for **AAL**, create a new column by applying `.pct_change()` to the **adjusted close** column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef10ba03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aal['returns'] = ...\n",
    "aal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c5ec35",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cb02d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we can get the log returns by adjusting the percent change function\n",
    "\n",
    "aal['logreturns'] = ...\n",
    "aal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6502c274",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eb5c80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# .cumsum() on a series will add everything in the column up until that point\n",
    "# this will give us the log return from the start, to the current index\n",
    "aal['cum_logreturns'] = ...\n",
    "aal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae5ecb0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9e8b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas also comes equipped with some built in plotting\n",
    "# we can plot the adjusted close, as well as the logreturns cumsum, and see if they look the same (they should)\n",
    "aal['Adj Close'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef464fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "aal['cum_logreturns'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1dde33",
   "metadata": {},
   "source": [
    "# Basic trend strategy: long/short by percentile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c3454a-df5b-491c-a1fa-033907365e5d",
   "metadata": {},
   "source": [
    "---\n",
    "### Developing a Trading Strategy\n",
    "\n",
    "Now, letâ€™s work on creating a **trading strategy** with the following assumptions:\n",
    "\n",
    "- **Shorting Allowed**: We assume that we can short stocks.\n",
    "- **Zero Borrowing Cost**: For simplicity, we assume the cost to borrow is **0** (note: this is a non-trivial assumption).\n",
    "\n",
    "#### Strategy Overview\n",
    "Our goal is to:\n",
    "- **Buy (go long)** stocks expected to increase in value.\n",
    "- **Sell (go short)** stocks expected to decrease in value.\n",
    "\n",
    "#### Approach\n",
    "1. **Ranking Stocks**:  \n",
    "   We will rank stocks based on a chosen **metric** (e.g., momentum, valuation ratios, or other factors).\n",
    "\n",
    "2. **Position Allocation**:  \n",
    "   - Go **long** on the top percentage of stocks ranked by the metric.\n",
    "   - Go **short** on the bottom percentage of stocks ranked by the metric.\n",
    "\n",
    "This approach allows us to systematically identify opportunities and construct a portfolio based on clear criteria.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d734274",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbd22c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to more easily group by asset, we'll make assets into its own column using df.stack()\n",
    "hdf_old = hdf.copy()\n",
    "hdf = (\n",
    "    hdf.stack()\n",
    "    .reset_index()\n",
    "    .rename(columns={'level_1': 'Symbol'})\n",
    ")\n",
    "hdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53442d2c-0cac-4273-aa9d-cfec7f4a526c",
   "metadata": {},
   "source": [
    "### Calculating Returns and Log Returns\n",
    "\n",
    "To compute **returns** and **log returns** for the entire dataset, we can **group by the ticker**, select the adjusted close column, and then use .pct_change() to get the percentage change from one row to the next\n",
    "\n",
    "These steps will provide the necessary data to derive both simple and log returns for analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a560a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hdf['returns'] = ...\n",
    "hdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd30be6b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b39229e-f3bf-4fb1-b11d-aa53b8fce96e",
   "metadata": {},
   "source": [
    "### Creating a Log Returns Column\n",
    "\n",
    "To calculate log returns, we'll create a new column in the dataset. This involves applying our **simple-to-log** function to the data and assigning the results to a column named `logreturns`.\n",
    "\n",
    "This new column will represent the log-transformed returns for each data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efe2657",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hdf['logreturns'] = ...\n",
    "hdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16dc6e5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270f2e18-25f4-4fac-9652-9897f2cb0ffd",
   "metadata": {},
   "source": [
    "### Creating a Forward Log Return Column\n",
    "\n",
    "To predict the next step of returns, we need to create a column for the **forward log return**. Here's how to do it:\n",
    "\n",
    "1. **Group by Ticker**: Group the data by the stock ticker to ensure calculations are performed within each stock's data.\n",
    "\n",
    "2. **Select Log Returns**: Focus on the column containing the log returns.\n",
    "\n",
    "3. **Shift the Series**: Use `.shift(-1)` to move the series **up one row**, aligning the future return with the current row.  \n",
    "   Refer to the [pandas.DataFrame.shift documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shift.html) for more details.\n",
    "\n",
    "This process creates a new column that represents the **forward log return**, which can be used as the target variable for prediction models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e51977b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hdf['fwd_logreturn'] = ...\n",
    "hdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d8424c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f692dbd-af8c-40b4-aa0d-1bf1dd00e6b7",
   "metadata": {},
   "source": [
    "### Dropping Rows with NaN Values\n",
    "\n",
    "To avoid issues in future operations, we need to drop rows where **log returns** or **forward log returns** contain `NaN` values. This can be done using the `subset` parameter in `df.dropna(subset=[])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7c6cd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hdf = hdf.dropna(subset=[...])\n",
    "hdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0006e1a3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6083b60a",
   "metadata": {},
   "source": [
    "# Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1d3ee8-ea50-4db5-a761-8f1204fb18de",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Ranking Returns by Asset\n",
    "\n",
    "To develop a strategy, we can **rank the returns** for each asset. \n",
    "\n",
    "#### Observations:\n",
    "- **Momentum Tendency**:  \n",
    "  We might guess that stocks that have **gone up** tend to continue rising, while stocks that have **gone down** tend to keep falling.  \n",
    "\n",
    "#### Why Might This Happen?  \n",
    "We might guess:\n",
    "- A stock that has risen significantly might attract buyers who want to **join the trend**, driving the price higher.\n",
    "- Conversely, a stock that has fallen may face additional selling pressure from investors exiting their positions.\n",
    "\n",
    "#### Strategy:\n",
    "- **Buy (Go Long)**: Identify and buy the stocks that have risen the most.  \n",
    "- **Sell (Go Short)**: Identify and short the stocks that have fallen the most.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ac8e8e-24b4-429e-b870-734c5bf6a3e5",
   "metadata": {},
   "source": [
    "\n",
    "### Calculating Ranks Using `.rank()`\n",
    "\n",
    "To calculate ranks for log returns, we can use the `.rank()` method. This method assigns a rank value as if the DataFrame were sorted. Key steps:\n",
    "\n",
    "1. **Descending Order**: Use `ascending=False` to rank in descending order.\n",
    "2. **Dense Ranking**: Specify `method='dense'` to ensure that ranks are consecutive even when there are ties.\n",
    "3. **Group by Date**: Group the data by date to calculate ranks within each group.\n",
    "4. **Assign to a New Column**: Create a new column, `logreturn_rank`, to store the ranks of the `logreturns` column.\n",
    "\n",
    "Refer to the [pandas.DataFrame.rank documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rank.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92688dab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hdf['logreturn_rank'] = ...\n",
    "hdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952e75e3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4f44e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we can also look at the demeaned returns, or the returns of the asset we chose relative to all assets\n",
    "# this will help us assess if we were able to choose the assets that would end up under/overperforming\n",
    "\n",
    "date_mean_logreturns = hdf.groupby('Date')['logreturns'].transform('mean')\n",
    "\n",
    "# Calculate the demeaned log returns\n",
    "hdf['demeaned_logreturn'] = hdf['logreturns'] - date_mean_logreturns\n",
    "\n",
    "# Similar to the above, calculate the mean *forward* log returns for each date\n",
    "date_mean_fwd_logreturns = ...\n",
    "\n",
    "# Calculate the demeaned forward log returns\n",
    "hdf['demeaned_fwd_logreturn'] = ...\n",
    "hdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465f196d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0b8f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ease of analyzing the effect, we will bucket our feature using a decile (each bin is 10%-ile)\n",
    "# this will help us figure out how strong the effect is\n",
    "# we do this using pd.qcut(x, q=quantile, labels=False, duplicates='drop'), which cuts a series into quantile buckets. pass q=10 to get decile buckets\n",
    "# [https://pandas.pydata.org/docs/reference/api/pandas.qcut.html]\n",
    "# drop labels and duplicates\n",
    "hdf['logreturn_decile'] = hdf.groupby('Date')['logreturns'].transform(\n",
    "    lambda x: pd.qcut(x, q=10, labels=False, duplicates='drop'))\n",
    "hdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98354b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll plot this using a barplot\n",
    "hdf.groupby('logreturn_decile').mean(numeric_only = True)['demeaned_fwd_logreturn'].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b498e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it definitely looks like the bottom and top 20%-ile have outsized returns!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edba518d",
   "metadata": {},
   "source": [
    "# Strategy 'backtesting'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44419fb5-f2b9-45cc-b8c8-fbb125c9b83e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Backtesting: A Simplified Approach\n",
    "\n",
    "\n",
    "This simplified backtest provides a quick way to assess the viability of features before diving into more complex analyses.\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb62af09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's define our strategy by setting an asset to be long or short if its in the top or bottom 20% log return decile\n",
    "# we'll also equal weight all our positions\n",
    "# the decile values for the bottom 20% would be 0 and 1\n",
    "# the decile values for the top 20% would be 8 and 9\n",
    "\n",
    "# we can use np.where() in order to conditionally assign a column\n",
    "hdf['long/short'] = ...\n",
    "hdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d8548b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b6da4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we size our positions by taking our absolute long/short position\n",
    "hdf['absposition'] = ...\n",
    "hdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5012de31",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925fb25b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# getting the total number of our positions by summing our absolute position for each day \n",
    "# hint: take a look at .transform documentation [https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.transform.html]\n",
    "hdf['numpositions'] = ...\n",
    "hdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977f8bde",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f452fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# and we get our final weight for each asset by scaling our long/short indicator variable by the number of positions we have\n",
    "# each position should be such that we add up to one, so we'd divide the indicator by total positions\n",
    "# if we have 0 positions, the weight should be 0\n",
    "# this will mean that we should have equal size long and short, adding up to a total of 1 (no leverage)\n",
    "# we'll use df.apply(lambda row: (expression using row) if (condition on some row) else value, axis=1)\n",
    "\n",
    "hdf['weight'] = ...\n",
    "hdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e6c4a7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6090a27f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# finally we define the strategy's return as the weighted logreturns based on our position\n",
    "# so we multiply weight by forward logreturn\n",
    "hdf['strategy_logreturn'] = ...\n",
    "hdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0054f4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4332f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we can compare this with an equal weighted strategy, where we long each asset the same amount, again summing up to 1\n",
    "# hint: use .unique() to get number of assets [https://pandas.pydata.org/docs/reference/api/pandas.unique.html]\n",
    "\n",
    "hdf['eqweight'] = ...\n",
    "hdf['equal_logreturn'] = ...\n",
    "hdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae0e674",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7690bd67",
   "metadata": {},
   "source": [
    "# Evaluating performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966ce1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf.groupby('Date').sum()['strategy_logreturn'].cumsum().plot(legend=True)\n",
    "hdf.groupby('Date').sum()['equal_logreturn'].cumsum().plot(legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c549ef3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we can get the final return by getting the last value of the column (values[-1])\n",
    "# and we can translate that to simple returns for readability\n",
    "final_strategyreturn = logtosimple(hdf.groupby('Date').sum()['strategy_logreturn'].cumsum().values[-1])\n",
    "final_equalreturn = ...\n",
    "\n",
    "print(f'final_strategyreturn: {final_strategyreturn}')\n",
    "print(f'...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8a60ba",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2838f37e-8a5c-4124-9a59-41e53429e59d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Evaluating Strategy Performance\n",
    "\n",
    "The equal-weight strategy significantly outperformed our strategy. Given the simplicity of our approach, this outcome is not unexpected. However, the equal-weight strategy also exhibited notable **volatility**. \n",
    "\n",
    "To better account for volatility, we can use the **Sharpe Ratio**. \n",
    "\n",
    "#### Understanding the Sharpe Ratio\n",
    "The Sharpe Ratio measures the **risk-adjusted return** of a strategy and can be thought of as a t-test for the statistical significance of the returns. It is calculated as:  \n",
    "\n",
    "$$ \\text{Sharpe Ratio} = \\frac{\\text{Mean Return} - \\text{Risk-Free Rate}}{\\text{Standard Deviation of Returns}}$$\n",
    "\n",
    "Benerally, any sharpe ratio over 1 is good, 2 is very good, 3+ is very very good. As you see a sharpe ratio > 3, the more likely that the strategy is somehow limited, or you've calculated something wrong\n",
    "\n",
    "#### Important Caveats\n",
    "In this example, the calculated Sharpe Ratios are **not realistic** due to several simplifications. For instance:\n",
    "- **Lookahead Bias**: Restricting the universe to companies currently in the S&P 500 introduces bias, as we are using information unavailable in past periods (e.g., in 2023).\n",
    "\n",
    "There are likely other ways this sample or testing strategy might be limited. Consider exploring additional potential biases or flaws in the approach!\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375a37d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's calculate the sharpe ratio with a function, we'll leave out the risk free rate part of it for now\n",
    "# we also need to normalize the sharpe ratio with respect to a year, by multiplying by the square root of periods our strategy trades in a year\n",
    "# note that there are 252 trading days\n",
    "\n",
    "def sharpe_ratio(mean_ret, std_ret):\n",
    "    return ... * np.sqrt(...)\n",
    "    \n",
    "strat_sharpe = sharpe_ratio(hdf.groupby('Date').sum()['strategy_logreturn'].mean(), \n",
    "             hdf.groupby('Date').sum()['strategy_logreturn'].std())\n",
    "equal_sharpe = sharpe_ratio(..., \n",
    "             ...)\n",
    "print(strat_sharpe, equal_sharpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dee441",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa597f92-fea5-4ecb-8063-cadb7d095f60",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Observations on Strategy Performance\n",
    "\n",
    "Interestingly, the equal-weight strategy showed a very high Sharpe Ratio. This is likely due to the period from October 2023 to March 2024 being particularly bullish. You can verify this by checking the S&P 500 returns during that time.\n",
    "\n",
    "To minimize the impact of such market-specific idiosyncrasies, it is generally better to backtest over a longer time horizon, such as a year or more. This helps reduce variance and provides a more robust evaluation of the strategy.\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4da9a4c",
   "metadata": {},
   "source": [
    "# That's it for sp25!\n",
    "This is all we recommend you complete this semester.\n",
    "If you're interested, you may keep doing the next cells. However, because the yfinance data changed, the test cases below will fail, so please ignore them. Again, the below is all optional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c998c9b",
   "metadata": {},
   "source": [
    "# accounting for fees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec900333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is without fees so it is clearly way too good\n",
    "# let's add a fee for each trade, and expected slippage per trade\n",
    "# the fee is what we would pay to the broker, and the expected slippage is likely a function of our position size\n",
    "# we'll combine these into one value, and just observe how our strategy decays as a function of cost\n",
    "\n",
    "# let's define a percentage fee per trade (e.g., 0.02%)\n",
    "fee = 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a493af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# in order to see how large our trade would be, we have to find the difference between our previous and current position size\n",
    "# we should sort values by symbol, then by the date\n",
    "hdf = hdf.sort_values(by=[...]) # order matters!\n",
    "\n",
    "# we'll groupby symbol, and then get the previous weight by using .shift(1) to shift the weights down\n",
    "hdf['prevweight'] = hdf.groupby('Symbol')['weight'] ...\n",
    "\n",
    "# next, we'll get the strategy's weight change by taking the difference between weight and prevweight\n",
    "hdf['strategy_weightchange'] = ...\n",
    "\n",
    "# finally, to calculate fees, we'll need to multiply the fee by our absolute change in position\n",
    "hdf['strategy_fees'] = abs(hdf['strategy_weightchange']) * fee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3508b4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q8a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c976ceb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now we'll calculate the strategy log return after fees by subtracting the groupby fees from the groupby logreturn\n",
    "hdf['strategy_postfees'] = ...\n",
    "strategy_postfees_seriestoplot = ...\n",
    "strategy_postfees_seriestoplot.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e314861",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8162cd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# and we'll test the sharpe and logreturn as above\n",
    "fees_sharpe = sharpe_ratio(..., \n",
    "                           ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6969901c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q8c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419cdfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the equal weight buy and hold does not change with fees, as it never changes position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c952ef",
   "metadata": {},
   "source": [
    "# expected returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620a0c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we have a very basic long/short strategy, we should try to improve upon it\n",
    "\n",
    "# our strategy roughly was equally long and short the market - regardless of how strongly something moved\n",
    "# even if all of the longs were very high return, the strategy didn't care\n",
    "# it also didn't care if it was 10th or 20th decile: we gave the same weight regardless\n",
    "# we might expect that there's a way to improve upon this\n",
    "\n",
    "# one way of doing this is trying to calculate an 'expected return' for each asset\n",
    "# this allows us to weight our positions based on how good we think they are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b956b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how might we make an expected returns model? \n",
    "# we'll likely want to fit to some historical data, and see how that strategy performs on data after that\n",
    "\n",
    "# to avoid our model just learning the optimal answer for our entire dataset\n",
    "# we'll train the model on the first 80% of our data\n",
    "# and see how it performs on the remaining 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffb9fb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split the data into training and testing using .iloc\n",
    "train_percent = ... # use 80% as a decimal\n",
    "# make sure to split according to time series!\n",
    "\n",
    "hdf = hdf.sort_values(...) # order matters! we need to split by time first, then asset\n",
    "\n",
    "# we can only use integer indices, so make sure to cast the value to an integer\n",
    "splitrow = ...\n",
    "training = hdf.iloc[:splitrow]\n",
    "testing = hdf.iloc[splitrow:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57b5106",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q9a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6604c6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a very common basic model is a linear regression: fitting a line to points of data\n",
    "# given some x variable, we try to solve for the optimal y = mx + b, \n",
    "# we do this by minimizing the squared sum of differences of our line to each data point\n",
    "# luckily there are libraries that do this for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105d7aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to run a linear regression on our training data, we need a data matrix X of features\n",
    "# and a target y to fit to \n",
    "# in our case, our target is forward log return\n",
    "# and our data matrix X is the current log returns\n",
    "# let's run the regression using statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae37b23b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature = [...]\n",
    "target = [...]\n",
    "\n",
    "# we add a constant to data matrix Xin order to get an intercept term, otherwise we would be fitting y = mx\n",
    "X = training[...]\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "y = training[...]\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(model.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ece38c4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q9b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17d40d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now we can plot our linear regression\n",
    "\n",
    "m = model.params[...]\n",
    "b = model.params[...]\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.scatter(training[feature], y, color='blue', label='Data')\n",
    "\n",
    "# Plot the linear regression line\n",
    "plt.plot(training[feature], m * training[feature] + b, color='red', label='linear regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef023de",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q9c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a119d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now we can see how well our model does on our testing data\n",
    "X_test = sm.add_constant(testing[feature])\n",
    "y_pred = model.predict(...).values\n",
    "\n",
    "# we compare the predictions to the actual values\n",
    "y_test = testing[...].values\n",
    "\n",
    "# we use mean squared error: the mean difference between y_test and y_pred, squared\n",
    "mse = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f0bd24",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q9d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad45e215",
   "metadata": {},
   "source": [
    "# updating backtest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74254ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also can look at our model's sharpe ratio on the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ada5da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# we can get the expected log returns by applying our model, with a constant, to the feature column\n",
    "\n",
    "testing['ex_logreturns'] = model.predict(sm.add_constant(...))\n",
    "\n",
    "# we'll size our weights according to the cross sectional predictions\n",
    "# use transform again to do this\n",
    "# we want to have each weight be (ex_logreturn-mean ex_logreturn for date)/(sum of absolute ex_logreturns for date)\n",
    "\n",
    "testing['ex_weight'] = testing.groupby('Date')['ex_logreturns'].transform(lambda x: ...)\n",
    "\n",
    "# and we multiply the forward log returns again\n",
    "testing['exstrategy_logreturns'] = ...\n",
    "\n",
    "# and we plot once more\n",
    "testing.groupby('Date').sum()['exstrategy_logreturns'].cumsum().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7b703d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q10a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b14bbff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# wow that looks very promising! let's calculate the sharpe ratio with fees\n",
    "# sort again\n",
    "testing = testing.sort_values(...)\n",
    "# get previous ex_weight as before\n",
    "testing['prevex_weight'] = ...\n",
    "# get change in weight as before\n",
    "testing['exstrategy_weightchange'] = ...\n",
    "\n",
    "# get absolute change in position\n",
    "testing['exstrategy_fees'] = ...\n",
    "# and find the return post fees\n",
    "testing['exstrategy_postfees'] = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fc54ea",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q10b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db56cb0",
   "metadata": {},
   "source": [
    "# feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279c4952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try to add more features to see if we can make this model any better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c453c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# first, let's create a function that makes the bar plot from before, so we can easily view any feature\n",
    "def summarize_feature(hdf, colname):\n",
    "    if hdf[colname].dtype == 'float':\n",
    "        # use qcut here, by deciles as before\n",
    "        bins = ...\n",
    "        hdf.groupby(bins).mean()['demeaned_fwd_logreturn'].plot(kind='bar')\n",
    "    else:\n",
    "        hdf.groupby(colname).mean()['demeaned_fwd_logreturn'].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3406af96",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q11a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f1e99e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's reincorporate sector data into this dataframe\n",
    "# we can do this using the merge function in pandas\n",
    "columns\n",
    "hdf = hdf.merge(df[[columns]], on='Symbol', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf3bf87",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q11b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d9da90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# another possible feature idea could be notional volume, or the dollar amount of shares traded\n",
    "# we can get notional volume by multiplying the close price with the shares traded\n",
    "hdf['ntlvolume'] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fd7188",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q11c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee7971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feel free to create more features here, and use the below functions to test them!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd24cb39",
   "metadata": {},
   "source": [
    "# testing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914afb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(hdf, features, train_test_split=0.8, debug=0):\n",
    "    # split the data into training and testing using .iloc\n",
    "    train_percent = 0.8\n",
    "    \n",
    "    # make sure to split according to time series!\n",
    "    hdf = hdf.sort_values(['Date', 'Symbol'])\n",
    "    training = hdf.iloc[:int(len(hdf) * train_percent)]\n",
    "    testing = hdf.iloc[int(len(hdf) * train_percent):]\n",
    "    \n",
    "    target = ['fwd_logreturn']\n",
    "    \n",
    "    # Identify categorical features based on data type\n",
    "    categorical_features = training[features].select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    # one hot encoding for categorical features\n",
    "    # we essentially create a bunch of extra columns, and assign them as ones and zeros\n",
    "    training = pd.get_dummies(training, columns=categorical_features)\n",
    "    testing = pd.get_dummies(testing, columns=categorical_features)\n",
    "    \n",
    "    # Update the features list to include dummy variables\n",
    "    features = [col for col in training.columns if col in features or col.startswith(tuple(categorical_features))]\n",
    "    if debug > 0:\n",
    "        print(f'features: {features}')\n",
    "    X_train = training[features]\n",
    "    X_train = sm.add_constant(X_train)\n",
    "    y_train = training[target]\n",
    "    \n",
    "    model = sm.OLS(y_train, X_train).fit()\n",
    "    if debug > 0:\n",
    "        print(f'r_squared: {model.rsquared}')\n",
    "    \n",
    "    # For testing data\n",
    "    X_test = testing[features]  # Include the same features used for training\n",
    "    X_test = sm.add_constant(X_test)\n",
    "    \n",
    "    return testing, model, X_test, debug  # Return testing data along with the model and testing features\n",
    "\n",
    "def basic_backtest(testing, model, X_test, debug, fee=0.0002):\n",
    "    testing['ex_logreturns'] = model.predict(X_test)  # Use X_test for prediction\n",
    "    testing['ex_weight'] = testing.groupby('Date')['ex_logreturns'].transform(lambda x: (x - x.mean()) / x.abs().sum())\n",
    "    testing['exstrategy_logreturns'] = testing['ex_weight'] * testing['fwd_logreturn']\n",
    "    testing = testing.sort_values(by=['Symbol', 'Date'])\n",
    "    testing['prevex_weight'] = testing.groupby('Symbol')['ex_weight'].shift(1)\n",
    "    testing['exstrategy_weightchange'] = testing['ex_weight'] - testing['prevex_weight']\n",
    "    testing['exstrategy_fees'] = abs(testing['exstrategy_weightchange']) * fee\n",
    "    testing['exstrategy_postfees'] = testing['strategy_logreturn'] - testing['exstrategy_fees']\n",
    "    if debug > 0:\n",
    "        testing.groupby('Date').sum()['exstrategy_fees']\n",
    "    testing.groupby('Date').sum()['exstrategy_postfees'].cumsum().plot()\n",
    "    print(f\"sharpe_ratio: {sharpe_ratio(testing.groupby('Date').sum()['exstrategy_postfees'].mean(), testing.groupby('Date').sum()['exstrategy_postfees'].std())}\")\n",
    "    if debug > 1:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        for symbol in testing['Symbol'].unique():\n",
    "#             print(symbol)\n",
    "            asset_weights = testing[testing['Symbol'] == symbol].set_index('Date')['ex_weight']\n",
    "            asset_weights.plot(label=symbol)\n",
    "        plt.title('Rolling Weights of Assets')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Weight')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5772688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the debug levels to get more or less information\n",
    "# pass your features into the list here\n",
    "# be very careful when changing the above functions!\n",
    "features = ['logreturns']\n",
    "basic_backtest(*(load_model(hdf, features, debug=1)), fee=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631d72b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there's much more to cover in the realm of researching and testing systematic trading strategies\n",
    "# importantly, we haven't covered much about risk modeling, and portfolio optimization \n",
    "# check out https://www.alacra.com/alacra/help/barra_handbook_US.pdf \n",
    "# for a good introduction to risk modeling, if you're curious!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74be3a38",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**\n",
    "\n",
    "Please also check gradescope for any written assignments for this week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f07a31",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cce917",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q10a": {
     "name": "q10a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(testing['exstrategy_logreturns'].mean(), 1.0620373849221123e-06)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q10b": {
     "name": "q10b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(np.abs(testing['exstrategy_postfees'].mean()), 9.185534624020494e-07)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q11a": {
     "name": "q11a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(hdf.groupby('logreturns').mean()['demeaned_fwd_logreturn'].max(), 0.29863292329834545)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q11b": {
     "name": "q11b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert hdf.shape == (56885, 24)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q11c": {
     "name": "q11c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(hdf['ntlvolume'].mean(),570866467.8644993)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1a": {
     "name": "q1a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert len(df_head5) == 5\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1b": {
     "name": "q1b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert len(columns) == 3\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1c": {
     "name": "q1c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert unique_sectors == 11\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1d": {
     "name": "q1d",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert it_sectors == 65\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> it_sectors\n65",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1e": {
     "name": "q1e",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert specified_slice.shape == (5,6)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1f": {
     "name": "q1f",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert false_healthcare == 439\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1g": {
     "name": "q1g",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert energy['CIK'].sum() == 19373487\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1h": {
     "name": "q1h",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert first4cols_industrials.shape == (79,4)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1i": {
     "name": "q1i",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert old_3_security == 'Bath & Body Works, Inc.'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1j": {
     "name": "q1j",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert np.isclose(mean_realestate_cik, 956436.7741935484)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2a": {
     "name": "q2a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert hdf.shape == hdf.shape\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2b": {
     "name": "q2b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert hdf.columns.size == 1006\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2c": {
     "name": "q2c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert broken_df.shape[1] == 8\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2d": {
     "name": "q2d",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert hdf.shape == (115, 998)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3a": {
     "name": "q3a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert np.isclose(logtosimple(1),1.718281828459045)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3b": {
     "name": "q3b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert np.isclose(simpletolog(1),0.6931471805599453)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3c": {
     "name": "q3c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert aal.shape == (115,2)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3d": {
     "name": "q3d",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert list(aal.columns.get_level_values(0).values) == ['Adj Close', 'Volume']\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3e": {
     "name": "q3e",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(aal['returns'].mean(),0.0011150589519497269)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3f": {
     "name": "q3f",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(aal['logreturns'].mean(),0.0007902055534179764)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3g": {
     "name": "q3g",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(aal['cum_logreturns'].mean(),0.05021465552608881)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4a": {
     "name": "q4a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(hdf['returns'].mean(), 0.0017457801190514555)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4b": {
     "name": "q4b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(hdf['logreturns'].mean(), 0.0015072870289151654)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4c": {
     "name": "q4c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(np.abs((hdf['logreturns'] - hdf['fwd_logreturn']).mean()),7.94025278927628e-05)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4d": {
     "name": "q4d",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert hdf.shape == (56161, 7)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5a": {
     "name": "q5a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert hdf['logreturn_rank'].values[0] == 392.0\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5b": {
     "name": "q5b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(np.abs(hdf['demeaned_fwd_logreturn'].mean()),\n...                np.abs((hdf['fwd_logreturn'] - date_mean_fwd_logreturns).mean()))\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6a": {
     "name": "q6a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(np.abs(hdf['long/short'].sum()), 1)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6b": {
     "name": "q6b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(hdf['absposition'].sum(),23116)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6c": {
     "name": "q6c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(hdf['numpositions'].sum(),11580916)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6d": {
     "name": "q6d",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(np.abs(hdf['weight'].mean()), 8.745933687456209e-08)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6e": {
     "name": "q6e",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(np.abs(hdf['strategy_logreturn'].mean()), 8.559054707207952e-07)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6f": {
     "name": "q6f",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(hdf['equal_logreturn'].mean(), 3.1830282614279954e-06)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7a": {
     "name": "q7a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(final_equalreturn, 0.19573618511585034)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7c": {
     "name": "q7c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(equal_sharpe, 3.1044873685151626)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q8a": {
     "name": "q8a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(equal_sharpe, 3.0157625925259532)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q8b": {
     "name": "q8b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(equal_sharpe, 3.0157625925259532)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q8c": {
     "name": "q8c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(equal_sharpe, 3.0436630129744557)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q9a": {
     "name": "q9a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert training.shape == (45508, 22)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q9b": {
     "name": "q9b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> model.rsquared\n0.05403784673350487",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert isclose(model.rsquared, 0.05333756613777707)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q9c": {
     "name": "q9c",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(model.params['logreturns'], 0.2324590858601042)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> assert isclose(model.params['const'], 0.0011427008449693891)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q9d": {
     "name": "q9d",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isclose(mse, 0.0002948418585430239)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
